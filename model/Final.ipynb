{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GXiLwsbaPUK",
        "outputId": "a6f35eb5-0c0a-4407-af92-d52babb96062"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjAv8LzNTLak",
        "outputId": "8b76df5b-cc75-4f8b-fd0c-423c36f3c48d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scipy tensorflow soundfile\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.fftpack\n",
        "import scipy.signal\n",
        "import scipy.io.wavfile as wav\n",
        "import soundfile as sf\n",
        "\n",
        "# Constants\n",
        "N_FFT = 512  # FFT Window Size\n",
        "HOP_LENGTH = 256  # Step Size for FFT\n",
        "N_MEL = 40  # Number of Mel Filters\n",
        "N_MFCC = 40  # Number of MFCC Features\n",
        "\n",
        "# Mel Filterbank Calculation (Precomputed)\n",
        "def mel_filterbank(num_filters, fft_size, sample_rate):\n",
        "    min_freq = 0\n",
        "    max_freq = sample_rate // 2\n",
        "    mel_min = 2595 * np.log10(1 + min_freq / 700)\n",
        "    mel_max = 2595 * np.log10(1 + max_freq / 700)\n",
        "    mel_points = np.linspace(mel_min, mel_max, num_filters + 2)\n",
        "    hz_points = 700 * (10**(mel_points / 2595) - 1)\n",
        "    bin_points = np.floor((fft_size + 1) * hz_points / sample_rate).astype(int)\n",
        "\n",
        "    filters = np.zeros((num_filters, fft_size // 2 + 1))\n",
        "    for i in range(1, num_filters + 1):\n",
        "        filters[i - 1, bin_points[i - 1]:bin_points[i]] = np.linspace(0, 1, bin_points[i] - bin_points[i - 1])\n",
        "        filters[i - 1, bin_points[i]:bin_points[i + 1]] = np.linspace(1, 0, bin_points[i + 1] - bin_points[i])\n",
        "    return filters\n",
        "\n",
        "# Function to Compute MFCC Features from WAV File\n",
        "def extract_mfcc(audio_file, num_mfcc=40):\n",
        "    # Load WAV File\n",
        "    sample_rate, signal = wav.read(audio_file)\n",
        "    if len(signal.shape) > 1:\n",
        "        signal = np.mean(signal, axis=1)  # Convert to Mono if Stereo\n",
        "\n",
        "    # Step 1: Compute Short-Time Fourier Transform (STFT)\n",
        "    _, _, stft_output = scipy.signal.stft(signal, fs=sample_rate, nperseg=N_FFT, noverlap=HOP_LENGTH)\n",
        "\n",
        "    # Step 2: Compute Spectrogram (Magnitude of STFT)\n",
        "    spectrogram = np.abs(stft_output)\n",
        "\n",
        "    # Step 3: Apply Mel Filterbank\n",
        "    mel_filters = mel_filterbank(N_MEL, N_FFT, sample_rate)\n",
        "    mel_spectrogram = np.dot(mel_filters, spectrogram)\n",
        "\n",
        "    # Step 4: Convert to Log Scale\n",
        "    log_mel_spectrogram = np.log(mel_spectrogram + 1e-10)\n",
        "\n",
        "    # Step 5: Apply Discrete Cosine Transform (DCT) to Get MFCCs\n",
        "    mfcc = scipy.fftpack.dct(log_mel_spectrogram, axis=0, norm='ortho')[:num_mfcc]\n",
        "\n",
        "    # Take Mean Across Time to Get a Fixed-Length Feature Vector\n",
        "    return np.mean(mfcc, axis=1)\n"
      ],
      "metadata": {
        "id": "zvNtg77xToow"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models\n",
        "import scipy.fftpack\n",
        "import scipy.signal\n",
        "\n",
        "# Constants\n",
        "N_FFT = 512  # FFT Window Size\n",
        "HOP_LENGTH = 256  # Step Size for FFT\n",
        "N_MEL = 40  # Number of Mel Filters\n",
        "N_MFCC = 40  # Number of MFCC Features\n",
        "\n",
        "# Load TensorFlow's Speech Commands Dataset\n",
        "dataset_name = \"speech_commands\"\n",
        "dataset, info = tfds.load(dataset_name, with_info=True, as_supervised=True, split=[\"train\", \"test\"])\n",
        "\n",
        "# List of desired keywords\n",
        "KEYWORDS = [\"up\", \"down\", \"left\", \"right\", \"stop\", \"go\"]\n",
        "KEYWORD_MAP = {word: i for i, word in enumerate(KEYWORDS)}\n",
        "\n",
        "# Mel Filterbank Calculation\n",
        "def mel_filterbank(num_filters, fft_size, sample_rate):\n",
        "    min_freq = 0\n",
        "    max_freq = sample_rate // 2\n",
        "    mel_min = 2595 * np.log10(1 + min_freq / 700)\n",
        "    mel_max = 2595 * np.log10(1 + max_freq / 700)\n",
        "    mel_points = np.linspace(mel_min, mel_max, num_filters + 2)\n",
        "    hz_points = 700 * (10**(mel_points / 2595) - 1)\n",
        "    bin_points = np.floor((fft_size + 1) * hz_points / sample_rate).astype(int)\n",
        "\n",
        "    filters = np.zeros((num_filters, fft_size // 2 + 1))\n",
        "    for i in range(1, num_filters + 1):\n",
        "        filters[i - 1, bin_points[i - 1]:bin_points[i]] = np.linspace(0, 1, bin_points[i] - bin_points[i - 1])\n",
        "        filters[i - 1, bin_points[i]:bin_points[i + 1]] = np.linspace(1, 0, bin_points[i + 1] - bin_points[i])\n",
        "    return filters\n",
        "\n",
        "# Function to Compute MFCC Features from Raw Audio\n",
        "def extract_mfcc(audio_array, num_mfcc=40, sample_rate=16000):\n",
        "    if len(audio_array.shape) > 1:\n",
        "        audio_array = np.mean(audio_array, axis=1)  # Convert to Mono if Stereo\n",
        "\n",
        "    # Compute Short-Time Fourier Transform (STFT)\n",
        "    _, _, stft_output = scipy.signal.stft(audio_array, fs=sample_rate, nperseg=N_FFT, noverlap=HOP_LENGTH)\n",
        "\n",
        "    # Compute Spectrogram (Magnitude of STFT)\n",
        "    spectrogram = np.abs(stft_output)\n",
        "\n",
        "    # Apply Mel Filterbank\n",
        "    mel_filters = mel_filterbank(N_MEL, N_FFT, sample_rate)\n",
        "    mel_spectrogram = np.dot(mel_filters, spectrogram)\n",
        "\n",
        "    # Convert to Log Scale\n",
        "    log_mel_spectrogram = np.log(mel_spectrogram + 1e-10)\n",
        "\n",
        "    # Apply Discrete Cosine Transform (DCT) to Get MFCCs\n",
        "    mfcc = scipy.fftpack.dct(log_mel_spectrogram, axis=0, norm='ortho')[:num_mfcc]\n",
        "\n",
        "    # Take Mean Across Time to Get a Fixed-Length Feature Vector\n",
        "    return np.mean(mfcc, axis=1)\n",
        "\n",
        "# Function to Process Dataset\n",
        "def filter_keywords(data):\n",
        "    mfcc_features_list = []\n",
        "    labels = []\n",
        "\n",
        "    for audio, label in tfds.as_numpy(data):\n",
        "        word = info.features[\"label\"].int2str(label)\n",
        "        if word in KEYWORDS:\n",
        "            # Convert TensorFlow tensor to NumPy array\n",
        "            audio_numpy = np.array(audio, dtype=np.float32)\n",
        "\n",
        "            # Extract MFCC features\n",
        "            mfcc_features = extract_mfcc(audio_numpy)\n",
        "            mfcc_features_list.append(mfcc_features)\n",
        "            labels.append(KEYWORD_MAP[word])\n",
        "\n",
        "    return np.array(mfcc_features_list), np.array(labels)\n",
        "\n",
        "# Extract MFCC Features from Training and Test Data\n",
        "X_train, y_train = filter_keywords(dataset[0])  # Train Set\n",
        "X_test, y_test = filter_keywords(dataset[1])  # Test Set\n",
        "\n",
        "# Normalize Data (Standardize MFCCs)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert Labels to One-Hot Encoding (Required for Classification)\n",
        "y_train = to_categorical(y_train, num_classes=len(KEYWORDS))\n",
        "y_test = to_categorical(y_test, num_classes=len(KEYWORDS))\n",
        "\n",
        "# Define Neural Network Model\n",
        "def create_model(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Dense(128, activation='relu', kernel_regularizer='l2'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile and Train Model\n",
        "model = create_model(input_shape=(40,), num_classes=len(KEYWORDS))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate Model on Test Set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7uTV6isZXlx",
        "outputId": "8833895f-6752-4064-bec4-9957264c5c7c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2500 - loss: 2.8971 - val_accuracy: 0.4726 - val_loss: 1.8152\n",
            "Epoch 2/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4369 - loss: 1.7746 - val_accuracy: 0.4950 - val_loss: 1.5095\n",
            "Epoch 3/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4642 - loss: 1.5446 - val_accuracy: 0.5018 - val_loss: 1.4386\n",
            "Epoch 4/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4779 - loss: 1.4814 - val_accuracy: 0.5099 - val_loss: 1.4167\n",
            "Epoch 5/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4778 - loss: 1.4760 - val_accuracy: 0.5023 - val_loss: 1.4112\n",
            "Epoch 6/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4862 - loss: 1.4603 - val_accuracy: 0.5089 - val_loss: 1.3995\n",
            "Epoch 7/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4845 - loss: 1.4579 - val_accuracy: 0.5045 - val_loss: 1.4012\n",
            "Epoch 8/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4842 - loss: 1.4533 - val_accuracy: 0.5143 - val_loss: 1.3877\n",
            "Epoch 9/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4840 - loss: 1.4557 - val_accuracy: 0.4963 - val_loss: 1.4011\n",
            "Epoch 10/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4874 - loss: 1.4379 - val_accuracy: 0.5181 - val_loss: 1.3817\n",
            "Epoch 11/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4910 - loss: 1.4394 - val_accuracy: 0.5211 - val_loss: 1.3817\n",
            "Epoch 12/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5002 - loss: 1.4298 - val_accuracy: 0.5230 - val_loss: 1.3778\n",
            "Epoch 13/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4921 - loss: 1.4317 - val_accuracy: 0.5296 - val_loss: 1.3689\n",
            "Epoch 14/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4898 - loss: 1.4295 - val_accuracy: 0.5247 - val_loss: 1.3591\n",
            "Epoch 15/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5068 - loss: 1.4133 - val_accuracy: 0.5170 - val_loss: 1.3708\n",
            "Epoch 16/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5088 - loss: 1.4157 - val_accuracy: 0.5263 - val_loss: 1.3633\n",
            "Epoch 17/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4960 - loss: 1.4229 - val_accuracy: 0.5268 - val_loss: 1.3587\n",
            "Epoch 18/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4926 - loss: 1.4312 - val_accuracy: 0.5334 - val_loss: 1.3549\n",
            "Epoch 19/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5080 - loss: 1.4106 - val_accuracy: 0.5276 - val_loss: 1.3653\n",
            "Epoch 20/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5027 - loss: 1.4173 - val_accuracy: 0.5364 - val_loss: 1.3457\n",
            "Epoch 21/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5048 - loss: 1.4028 - val_accuracy: 0.5334 - val_loss: 1.3570\n",
            "Epoch 22/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5063 - loss: 1.4139 - val_accuracy: 0.5293 - val_loss: 1.3479\n",
            "Epoch 23/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5075 - loss: 1.4118 - val_accuracy: 0.5257 - val_loss: 1.3523\n",
            "Epoch 24/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5049 - loss: 1.4088 - val_accuracy: 0.5244 - val_loss: 1.3519\n",
            "Epoch 25/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5074 - loss: 1.4166 - val_accuracy: 0.5309 - val_loss: 1.3501\n",
            "Epoch 26/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5016 - loss: 1.4120 - val_accuracy: 0.5342 - val_loss: 1.3434\n",
            "Epoch 27/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5033 - loss: 1.4235 - val_accuracy: 0.5328 - val_loss: 1.3421\n",
            "Epoch 28/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5048 - loss: 1.4066 - val_accuracy: 0.5298 - val_loss: 1.3454\n",
            "Epoch 29/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5090 - loss: 1.4072 - val_accuracy: 0.5336 - val_loss: 1.3526\n",
            "Epoch 30/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5069 - loss: 1.4091 - val_accuracy: 0.5350 - val_loss: 1.3419\n",
            "Epoch 31/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5103 - loss: 1.4086 - val_accuracy: 0.5304 - val_loss: 1.3397\n",
            "Epoch 32/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5098 - loss: 1.4072 - val_accuracy: 0.5317 - val_loss: 1.3430\n",
            "Epoch 33/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5041 - loss: 1.4124 - val_accuracy: 0.5309 - val_loss: 1.3394\n",
            "Epoch 34/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5089 - loss: 1.4003 - val_accuracy: 0.5276 - val_loss: 1.3455\n",
            "Epoch 35/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5065 - loss: 1.4055 - val_accuracy: 0.5388 - val_loss: 1.3358\n",
            "Epoch 36/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5172 - loss: 1.3865 - val_accuracy: 0.5350 - val_loss: 1.3467\n",
            "Epoch 37/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5094 - loss: 1.4021 - val_accuracy: 0.5375 - val_loss: 1.3315\n",
            "Epoch 38/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5140 - loss: 1.3984 - val_accuracy: 0.5369 - val_loss: 1.3322\n",
            "Epoch 39/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5066 - loss: 1.4088 - val_accuracy: 0.5361 - val_loss: 1.3530\n",
            "Epoch 40/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5081 - loss: 1.3975 - val_accuracy: 0.5312 - val_loss: 1.3464\n",
            "Epoch 41/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5133 - loss: 1.3952 - val_accuracy: 0.5377 - val_loss: 1.3402\n",
            "Epoch 42/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5130 - loss: 1.4003 - val_accuracy: 0.5383 - val_loss: 1.3371\n",
            "Epoch 43/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5119 - loss: 1.4004 - val_accuracy: 0.5421 - val_loss: 1.3270\n",
            "Epoch 44/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5070 - loss: 1.4062 - val_accuracy: 0.5424 - val_loss: 1.3370\n",
            "Epoch 45/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5154 - loss: 1.4022 - val_accuracy: 0.5355 - val_loss: 1.3397\n",
            "Epoch 46/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5086 - loss: 1.3993 - val_accuracy: 0.5380 - val_loss: 1.3322\n",
            "Epoch 47/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5143 - loss: 1.4005 - val_accuracy: 0.5448 - val_loss: 1.3228\n",
            "Epoch 48/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5218 - loss: 1.3824 - val_accuracy: 0.5415 - val_loss: 1.3354\n",
            "Epoch 49/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5168 - loss: 1.3905 - val_accuracy: 0.5350 - val_loss: 1.3381\n",
            "Epoch 50/50\n",
            "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5191 - loss: 1.3983 - val_accuracy: 0.5399 - val_loss: 1.3386\n",
            "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5243 - loss: 1.4002\n",
            "Test Accuracy: 0.5252854824066162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models\n",
        "import scipy.fftpack\n",
        "import scipy.signal\n",
        "\n",
        "# Constants\n",
        "N_FFT = 512  # FFT Window Size\n",
        "HOP_LENGTH = 256  # Step Size for FFT\n",
        "N_MEL = 40  # Number of Mel Filters\n",
        "N_MFCC = 40  # Number of MFCC Features\n",
        "\n",
        "# Load TensorFlow's Speech Commands Dataset\n",
        "dataset_name = \"speech_commands\"\n",
        "dataset, info = tfds.load(dataset_name, with_info=True, as_supervised=True, split=\"train\")\n",
        "\n",
        "# List of desired keywords\n",
        "KEYWORDS = [\"up\", \"down\", \"left\", \"right\", \"stop\", \"go\"]\n",
        "KEYWORD_MAP = {word: i for i, word in enumerate(KEYWORDS)}\n",
        "\n",
        "# Function to Compute MFCC Features from Raw Audio\n",
        "def extract_mfcc(audio_array, num_mfcc=40, sample_rate=16000):\n",
        "    if len(audio_array.shape) > 1:\n",
        "        audio_array = np.mean(audio_array, axis=1)  # Convert to Mono if Stereo\n",
        "\n",
        "    # Compute Short-Time Fourier Transform (STFT)\n",
        "    _, _, stft_output = scipy.signal.stft(audio_array, fs=sample_rate, nperseg=N_FFT, noverlap=HOP_LENGTH)\n",
        "\n",
        "    # Compute Spectrogram (Magnitude of STFT)\n",
        "    spectrogram = np.abs(stft_output)\n",
        "\n",
        "    # Apply Mel Filterbank\n",
        "    mel_filters = mel_filterbank(N_MEL, N_FFT, sample_rate)\n",
        "    mel_spectrogram = np.dot(mel_filters, spectrogram)\n",
        "\n",
        "    # Convert to Log Scale\n",
        "    log_mel_spectrogram = np.log(mel_spectrogram + 1e-10)\n",
        "\n",
        "    # Apply Discrete Cosine Transform (DCT) to Get MFCCs\n",
        "    mfcc = scipy.fftpack.dct(log_mel_spectrogram, axis=0, norm='ortho')[:num_mfcc]\n",
        "\n",
        "    # Take Mean Across Time to Get a Fixed-Length Feature Vector\n",
        "    return np.mean(mfcc, axis=1)\n",
        "\n",
        "# Function to Filter Only Selected Keywords\n",
        "def filter_keywords(data):\n",
        "    mfcc_features_list = []\n",
        "    labels = []\n",
        "\n",
        "    for audio, label in tfds.as_numpy(data):\n",
        "        word = info.features[\"label\"].int2str(label)\n",
        "        if word in KEYWORDS:  # Only process selected words\n",
        "            audio_numpy = np.array(audio, dtype=np.float32)\n",
        "            mfcc_features = extract_mfcc(audio_numpy)  # Extract MFCCs\n",
        "            mfcc_features_list.append(mfcc_features)\n",
        "            labels.append(KEYWORD_MAP[word])  # Assign label\n",
        "\n",
        "    return np.array(mfcc_features_list), np.array(labels)\n",
        "\n",
        "# Extract MFCC Features from Training Data (Only Selected Words)\n",
        "X_train, y_train = filter_keywords(dataset)  # Train Set\n",
        "\n",
        "# Normalize Data (Standardize MFCCs)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "# Convert Labels to One-Hot Encoding (Required for Classification)\n",
        "y_train = to_categorical(y_train, num_classes=len(KEYWORDS))\n",
        "\n",
        "# Split into Train/Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Neural Network Model\n",
        "def create_model(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Dense(128, activation='relu', kernel_regularizer='l2'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile and Train Model\n",
        "model = create_model(input_shape=(40,), num_classes=len(KEYWORDS))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate Model on Test Set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eNhqKu8ZyaI",
        "outputId": "016cc981-8b4a-4b08-ae8b-2f6970ac333c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2324 - loss: 2.9248 - val_accuracy: 0.4665 - val_loss: 1.9031\n",
            "Epoch 2/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4209 - loss: 1.8485 - val_accuracy: 0.4934 - val_loss: 1.5640\n",
            "Epoch 3/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4603 - loss: 1.5894 - val_accuracy: 0.4825 - val_loss: 1.4807\n",
            "Epoch 4/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.4646 - loss: 1.5169 - val_accuracy: 0.5063 - val_loss: 1.4448\n",
            "Epoch 5/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4752 - loss: 1.4805 - val_accuracy: 0.5015 - val_loss: 1.4331\n",
            "Epoch 6/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4826 - loss: 1.4695 - val_accuracy: 0.5029 - val_loss: 1.4206\n",
            "Epoch 7/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4813 - loss: 1.4660 - val_accuracy: 0.5073 - val_loss: 1.4144\n",
            "Epoch 8/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4863 - loss: 1.4532 - val_accuracy: 0.5128 - val_loss: 1.4011\n",
            "Epoch 9/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4950 - loss: 1.4392 - val_accuracy: 0.5138 - val_loss: 1.4003\n",
            "Epoch 10/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4901 - loss: 1.4529 - val_accuracy: 0.5060 - val_loss: 1.3938\n",
            "Epoch 11/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4906 - loss: 1.4394 - val_accuracy: 0.5073 - val_loss: 1.4036\n",
            "Epoch 12/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4983 - loss: 1.4322 - val_accuracy: 0.5114 - val_loss: 1.3866\n",
            "Epoch 13/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4853 - loss: 1.4455 - val_accuracy: 0.5138 - val_loss: 1.3941\n",
            "Epoch 14/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4947 - loss: 1.4411 - val_accuracy: 0.5254 - val_loss: 1.3775\n",
            "Epoch 15/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5005 - loss: 1.4270 - val_accuracy: 0.5281 - val_loss: 1.3770\n",
            "Epoch 16/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4997 - loss: 1.4237 - val_accuracy: 0.5209 - val_loss: 1.3665\n",
            "Epoch 17/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4960 - loss: 1.4308 - val_accuracy: 0.5209 - val_loss: 1.3778\n",
            "Epoch 18/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5031 - loss: 1.4129 - val_accuracy: 0.5243 - val_loss: 1.3813\n",
            "Epoch 19/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.4961 - loss: 1.4173 - val_accuracy: 0.5332 - val_loss: 1.3616\n",
            "Epoch 20/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5011 - loss: 1.4235 - val_accuracy: 0.5308 - val_loss: 1.3680\n",
            "Epoch 21/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5061 - loss: 1.4131 - val_accuracy: 0.5250 - val_loss: 1.3649\n",
            "Epoch 22/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.5081 - loss: 1.4139 - val_accuracy: 0.5291 - val_loss: 1.3664\n",
            "Epoch 23/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5096 - loss: 1.4097 - val_accuracy: 0.5332 - val_loss: 1.3635\n",
            "Epoch 24/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5090 - loss: 1.4091 - val_accuracy: 0.5312 - val_loss: 1.3587\n",
            "Epoch 25/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5034 - loss: 1.4127 - val_accuracy: 0.5271 - val_loss: 1.3689\n",
            "Epoch 26/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5013 - loss: 1.4252 - val_accuracy: 0.5315 - val_loss: 1.3591\n",
            "Epoch 27/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5078 - loss: 1.4101 - val_accuracy: 0.5247 - val_loss: 1.3639\n",
            "Epoch 28/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5017 - loss: 1.4110 - val_accuracy: 0.5301 - val_loss: 1.3548\n",
            "Epoch 29/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.4980 - loss: 1.4133 - val_accuracy: 0.5308 - val_loss: 1.3609\n",
            "Epoch 30/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.5017 - loss: 1.4073 - val_accuracy: 0.5260 - val_loss: 1.3648\n",
            "Epoch 31/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.5080 - loss: 1.4035 - val_accuracy: 0.5308 - val_loss: 1.3503\n",
            "Epoch 32/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5148 - loss: 1.3981 - val_accuracy: 0.5424 - val_loss: 1.3525\n",
            "Epoch 33/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5160 - loss: 1.3896 - val_accuracy: 0.5322 - val_loss: 1.3589\n",
            "Epoch 34/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5100 - loss: 1.4011 - val_accuracy: 0.5349 - val_loss: 1.3466\n",
            "Epoch 35/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5139 - loss: 1.3966 - val_accuracy: 0.5410 - val_loss: 1.3473\n",
            "Epoch 36/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5076 - loss: 1.4039 - val_accuracy: 0.5318 - val_loss: 1.3534\n",
            "Epoch 37/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5104 - loss: 1.4109 - val_accuracy: 0.5335 - val_loss: 1.3512\n",
            "Epoch 38/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5022 - loss: 1.4112 - val_accuracy: 0.5230 - val_loss: 1.3722\n",
            "Epoch 39/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5098 - loss: 1.4032 - val_accuracy: 0.5363 - val_loss: 1.3477\n",
            "Epoch 40/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5101 - loss: 1.4051 - val_accuracy: 0.5220 - val_loss: 1.3564\n",
            "Epoch 41/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5093 - loss: 1.4046 - val_accuracy: 0.5369 - val_loss: 1.3456\n",
            "Epoch 42/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5088 - loss: 1.3961 - val_accuracy: 0.5438 - val_loss: 1.3401\n",
            "Epoch 43/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5143 - loss: 1.3927 - val_accuracy: 0.5390 - val_loss: 1.3456\n",
            "Epoch 44/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5163 - loss: 1.3994 - val_accuracy: 0.5420 - val_loss: 1.3458\n",
            "Epoch 45/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5108 - loss: 1.3918 - val_accuracy: 0.5342 - val_loss: 1.3540\n",
            "Epoch 46/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5127 - loss: 1.4062 - val_accuracy: 0.5414 - val_loss: 1.3489\n",
            "Epoch 47/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5164 - loss: 1.3933 - val_accuracy: 0.5438 - val_loss: 1.3469\n",
            "Epoch 48/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5195 - loss: 1.3854 - val_accuracy: 0.5458 - val_loss: 1.3358\n",
            "Epoch 49/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5190 - loss: 1.3936 - val_accuracy: 0.5424 - val_loss: 1.3480\n",
            "Epoch 50/50\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5071 - loss: 1.4078 - val_accuracy: 0.5346 - val_loss: 1.3450\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5223 - loss: 1.3846\n",
            "Test Accuracy: 0.5366384983062744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models\n",
        "import scipy.fftpack\n",
        "import scipy.signal\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Constants\n",
        "N_FFT = 512  # FFT Window Size\n",
        "HOP_LENGTH = 256  # Step Size for FFT\n",
        "N_MEL = 40  # Number of Mel Filters\n",
        "N_MFCC = 40  # Number of MFCC Features\n",
        "FIXED_TIME_STEPS = 32  # Fixed time steps for MFCCs\n",
        "\n",
        "# Load TensorFlow's Speech Commands Dataset\n",
        "dataset_name = \"speech_commands\"\n",
        "dataset, info = tfds.load(dataset_name, with_info=True, as_supervised=True, split=\"train\")\n",
        "\n",
        "# List of desired keywords\n",
        "KEYWORDS = [\"up\", \"down\", \"left\", \"right\", \"stop\", \"go\"]\n",
        "KEYWORD_MAP = {word: i for i, word in enumerate(KEYWORDS)}\n",
        "\n",
        "# Mel Filterbank Calculation\n",
        "def mel_filterbank(num_filters, fft_size, sample_rate):\n",
        "    min_freq = 0\n",
        "    max_freq = sample_rate // 2\n",
        "    mel_min = 2595 * np.log10(1 + min_freq / 700)\n",
        "    mel_max = 2595 * np.log10(1 + max_freq / 700)\n",
        "    mel_points = np.linspace(mel_min, mel_max, num_filters + 2)\n",
        "    hz_points = 700 * (10**(mel_points / 2595) - 1)\n",
        "    bin_points = np.floor((fft_size + 1) * hz_points / sample_rate).astype(int)\n",
        "\n",
        "    filters = np.zeros((num_filters, fft_size // 2 + 1))\n",
        "    for i in range(1, num_filters + 1):\n",
        "        filters[i - 1, bin_points[i - 1]:bin_points[i]] = np.linspace(0, 1, bin_points[i] - bin_points[i - 1])\n",
        "        filters[i - 1, bin_points[i]:bin_points[i + 1]] = np.linspace(1, 0, bin_points[i + 1] - bin_points[i])\n",
        "    return filters\n",
        "\n",
        "# Function to Compute MFCC Features from Raw Audio\n",
        "def extract_mfcc(audio_array, num_mfcc=40, sample_rate=16000):\n",
        "    if len(audio_array.shape) > 1:\n",
        "        audio_array = np.mean(audio_array, axis=1)  # Convert to Mono if Stereo\n",
        "\n",
        "    # Compute Short-Time Fourier Transform (STFT)\n",
        "    _, _, stft_output = scipy.signal.stft(audio_array, fs=sample_rate, nperseg=N_FFT, noverlap=HOP_LENGTH)\n",
        "\n",
        "    # Compute Spectrogram (Magnitude of STFT)\n",
        "    spectrogram = np.abs(stft_output)\n",
        "\n",
        "    # Apply Mel Filterbank\n",
        "    mel_filters = mel_filterbank(N_MEL, N_FFT, sample_rate)\n",
        "    mel_spectrogram = np.dot(mel_filters, spectrogram)\n",
        "\n",
        "    # Convert to Log Scale\n",
        "    log_mel_spectrogram = np.log(mel_spectrogram + 1e-10)\n",
        "\n",
        "    # Apply Discrete Cosine Transform (DCT) to Get MFCCs\n",
        "    mfcc = scipy.fftpack.dct(log_mel_spectrogram, axis=0, norm='ortho')[:num_mfcc]\n",
        "\n",
        "    # **Ensure Fixed Shape: Pad or Truncate**\n",
        "    if mfcc.shape[1] < FIXED_TIME_STEPS:  # If too short, pad with zeros\n",
        "        padding = np.zeros((num_mfcc, FIXED_TIME_STEPS - mfcc.shape[1]))\n",
        "        mfcc = np.hstack((mfcc, padding))\n",
        "    elif mfcc.shape[1] > FIXED_TIME_STEPS:  # If too long, truncate\n",
        "        mfcc = mfcc[:, :FIXED_TIME_STEPS]\n",
        "\n",
        "    return mfcc  # Returns shape (40, FIXED_TIME_STEPS)\n",
        "\n",
        "# Function to Filter Only Selected Keywords\n",
        "def filter_keywords(data):\n",
        "    mfcc_features_list = []\n",
        "    labels = []\n",
        "\n",
        "    for audio, label in tfds.as_numpy(data):\n",
        "        word = info.features[\"label\"].int2str(label)\n",
        "        if word in KEYWORDS:  # Only process selected words\n",
        "            audio_numpy = np.array(audio, dtype=np.float32)\n",
        "            mfcc_features = extract_mfcc(audio_numpy)  # Extract MFCCs\n",
        "            mfcc_features_list.append(mfcc_features)\n",
        "            labels.append(KEYWORD_MAP[word])  # Assign label\n",
        "\n",
        "    return np.array(mfcc_features_list), np.array(labels)\n",
        "\n",
        "# Extract MFCC Features from Training Data (Only Selected Words)\n",
        "X_train, y_train = filter_keywords(dataset)  # Train Set\n",
        "\n",
        "# Normalize Data (Standardize MFCCs)\n",
        "scaler = StandardScaler()\n",
        "X_train = np.array([scaler.fit_transform(mfcc.T).T for mfcc in X_train])  # Normalize across time axis\n",
        "\n",
        "# Convert Labels to One-Hot Encoding\n",
        "y_train = to_categorical(y_train, num_classes=len(KEYWORDS))\n",
        "\n",
        "# Reshape for CNN Input: (samples, time_steps, features)\n",
        "X_train = X_train.reshape(X_train.shape[0], FIXED_TIME_STEPS, N_MFCC, 1)  # Corrected shape\n",
        "\n",
        "# Split into Train/Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# **📌 Optimized CNN Model for FPGA**\n",
        "def create_fpga_friendly_cnn(input_shape, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(16, kernel_size=(3,3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),  # Small dropout to prevent overfitting\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile and Train Model\n",
        "model = create_fpga_friendly_cnn(input_shape=(FIXED_TIME_STEPS, N_MFCC, 1), num_classes=len(KEYWORDS))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate Model on Test Set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P64_afVugJUf",
        "outputId": "acee2e5f-f867-4de6-b056-bc44e6aba3db"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.1811 - loss: 1.7935 - val_accuracy: 0.2843 - val_loss: 1.6704\n",
            "Epoch 2/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.3368 - loss: 1.5656 - val_accuracy: 0.4848 - val_loss: 1.3213\n",
            "Epoch 3/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.4792 - loss: 1.3062 - val_accuracy: 0.5472 - val_loss: 1.1722\n",
            "Epoch 4/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.5666 - loss: 1.1201 - val_accuracy: 0.5860 - val_loss: 1.0886\n",
            "Epoch 5/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.6307 - loss: 0.9712 - val_accuracy: 0.6136 - val_loss: 1.0193\n",
            "Epoch 6/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.6744 - loss: 0.8671 - val_accuracy: 0.6061 - val_loss: 1.0185\n",
            "Epoch 7/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.7096 - loss: 0.7769 - val_accuracy: 0.6149 - val_loss: 1.0201\n",
            "Epoch 8/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.7451 - loss: 0.6777 - val_accuracy: 0.6210 - val_loss: 1.0260\n",
            "Epoch 9/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.7709 - loss: 0.6079 - val_accuracy: 0.6098 - val_loss: 1.0823\n",
            "Epoch 10/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.7977 - loss: 0.5458 - val_accuracy: 0.6234 - val_loss: 1.1160\n",
            "Epoch 11/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.8218 - loss: 0.4917 - val_accuracy: 0.6313 - val_loss: 1.1359\n",
            "Epoch 12/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.8411 - loss: 0.4324 - val_accuracy: 0.6112 - val_loss: 1.2069\n",
            "Epoch 13/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.8485 - loss: 0.3977 - val_accuracy: 0.6244 - val_loss: 1.2524\n",
            "Epoch 14/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.8629 - loss: 0.3696 - val_accuracy: 0.6159 - val_loss: 1.3639\n",
            "Epoch 15/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.8827 - loss: 0.3163 - val_accuracy: 0.6159 - val_loss: 1.4047\n",
            "Epoch 16/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.8835 - loss: 0.3096 - val_accuracy: 0.6153 - val_loss: 1.4215\n",
            "Epoch 17/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.8730 - loss: 0.3351 - val_accuracy: 0.6129 - val_loss: 1.4750\n",
            "Epoch 18/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.8882 - loss: 0.2902 - val_accuracy: 0.6136 - val_loss: 1.5437\n",
            "Epoch 19/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.8984 - loss: 0.2749 - val_accuracy: 0.6115 - val_loss: 1.5975\n",
            "Epoch 20/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.9039 - loss: 0.2491 - val_accuracy: 0.6047 - val_loss: 1.7525\n",
            "Epoch 21/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9062 - loss: 0.2529 - val_accuracy: 0.6129 - val_loss: 1.7131\n",
            "Epoch 22/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9180 - loss: 0.2164 - val_accuracy: 0.6095 - val_loss: 1.8059\n",
            "Epoch 23/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - accuracy: 0.9195 - loss: 0.2154 - val_accuracy: 0.6098 - val_loss: 1.7682\n",
            "Epoch 24/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.9245 - loss: 0.2042 - val_accuracy: 0.6084 - val_loss: 1.8372\n",
            "Epoch 25/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.9124 - loss: 0.2221 - val_accuracy: 0.6088 - val_loss: 1.8205\n",
            "Epoch 26/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.9175 - loss: 0.2232 - val_accuracy: 0.6156 - val_loss: 1.8621\n",
            "Epoch 27/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9232 - loss: 0.1941 - val_accuracy: 0.6071 - val_loss: 1.9409\n",
            "Epoch 28/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9190 - loss: 0.2151 - val_accuracy: 0.6054 - val_loss: 1.9576\n",
            "Epoch 29/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.9277 - loss: 0.1877 - val_accuracy: 0.6071 - val_loss: 2.0230\n",
            "Epoch 30/30\n",
            "\u001b[1m368/368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.9362 - loss: 0.1729 - val_accuracy: 0.6033 - val_loss: 2.3129\n",
            "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5694 - loss: 2.5394\n",
            "Test Accuracy: 0.5837646126747131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save feature scaler (mean & std deviation)\n",
        "scaler_filename = \"feature_scaler.pkl\"\n",
        "joblib.dump(scaler, scaler_filename)\n",
        "print(f\"Feature scaler saved as {scaler_filename}\")\n",
        "\n",
        "# To Load on FPGA:\n",
        "# scaler = joblib.load(\"feature_scaler.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97AsvD17ixRL",
        "outputId": "0e99eb7f-75d4-45d2-a8a3-933d1f408645"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature scaler saved as feature_scaler.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "# Extract MFCCs from a sample WAV file\n",
        "sample_wav = \"/content/UP1.wav\"  # Example file\n",
        "\n",
        "# **Load audio file correctly**\n",
        "audio_array, sr = librosa.load(sample_wav, sr=16000)  # Ensure 16kHz sample rate\n",
        "\n",
        "# Extract MFCC features\n",
        "sample_mfcc = extract_mfcc(audio_array, num_mfcc=40, sample_rate=sr)\n",
        "\n",
        "# Save as .npy file\n",
        "np.save(\"sample_mfcc.npy\", sample_mfcc)\n",
        "print(\"Sample MFCCs saved as sample_mfcc.npy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0HwmXzypf0U",
        "outputId": "7081af23-99b0-4ffb-e4a3-7f628ba2168e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample MFCCs saved as sample_mfcc.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to save layer weights & biases\n",
        "def save_layer_weights(model, layer_names):\n",
        "    for layer_name in layer_names:\n",
        "        layer = model.get_layer(name=layer_name)\n",
        "        weights, biases = layer.get_weights()\n",
        "\n",
        "        # Save weights (flattened for FPGA memory storage)\n",
        "        np.savetxt(f\"{layer_name}_weights.txt\", weights.flatten(), fmt=\"%.6f\")\n",
        "        # Save biases\n",
        "        np.savetxt(f\"{layer_name}_biases.txt\", biases.flatten(), fmt=\"%.6f\")\n",
        "\n",
        "        print(f\"Saved {layer_name} weights & biases\")\n",
        "\n",
        "# ✅ Use correct layer names from the model summary\n",
        "layer_names = [\"conv2d\", \"conv2d_1\", \"dense_6\", \"dense_7\"]\n",
        "\n",
        "# Save weights & biases\n",
        "save_layer_weights(model, layer_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CgCIxcEqrNQ",
        "outputId": "80082e79-5c16-4562-c79e-c8e55ac1f2b5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved conv2d weights & biases\n",
            "Saved conv2d_1 weights & biases\n",
            "Saved dense_6 weights & biases\n",
            "Saved dense_7 weights & biases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JW9a-mYYsx9I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}